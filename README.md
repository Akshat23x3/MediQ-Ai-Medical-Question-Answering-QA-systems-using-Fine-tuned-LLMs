Abstract-With the advent of advancement and development of Question Answering (QA) systems fitted and utilized in specific domains based on NLP techiques. These systems have proved to be of immense potential in the medical and healthcare field to assist some healthcare professionals such as doctors, nurses in securing relevant insights into the patient's disease efficiently. This paper introduces MediQ-Ai, a QA model designed specifically for the medical domain, leveraging the comprehensive MedQuad dataset. MediQ-Ai is built upon some of the latest and state-of-the-art NLP architectures which was further trained on the MedQuad dataset, which basically comprises a diverse range of medical questions and their corresponding answers sourced from reliable medical literature and expert annotations. This model employs an amalgamation of pre-trained language representations and some of the domain-specific fine-tuning techniques to achieve robust performance in medical OA tasks. Some of the MediQuadA's Key features include its ability to comprehend and decode complex medical terminology, in order to accurately interpret nuanced questions, and provide contex- tually appropriate answers. Through an extensive experimenta- tion and evaluation on the benchmark datasets, MedQuadQA demonstrates superior performance compared to other existing QA models, especially in tasks requiring a deep understanding of medical concepts and literature. We employ a diverse set of pre- trained language models, including BioGPT Causal, and LLAMA 2, GPT-2 and fine tune them to explore the effectiveness of various architectures in capturing the nuanced semantics and domain- specific knowledge prevalent in medical texts. One of the main aims of this paper is to present a viable approach to fine-tune models on resourcefully restricted systems like Colab Notebooks and Kaggle. Model performance and Evaluation metric tests such as ROUGE and Perplexity have been studied for the text generation task. The Rouge metric evaluates the recall of n-grams between the model outputs and expected answers, reflecting the

overlap accuracy, while perplexity measures the model's surprise

on encountering new data, indicating predictive fluency.
